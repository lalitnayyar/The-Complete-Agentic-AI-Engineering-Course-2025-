{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4 Day 4 - preparing the big project!\n",
    "\n",
    "# The Sidekick\n",
    "\n",
    "It's time to introduce:\n",
    "\n",
    "1. Structured Outputs\n",
    "2. A multi-agent flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict, List, Dict, Any, Optional\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.agent_toolkits import PlayWrightBrowserToolkit\n",
    "from langchain_community.tools.playwright.utils import create_async_playwright_browser\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Image, display\n",
    "import gradio as gr\n",
    "import uuid\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For structured outputs, we define a Pydantic object for the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define a structured output\n",
    "\n",
    "class EvaluatorOutput(BaseModel):\n",
    "    feedback: str = Field(description=\"Feedback on the assistant's response\")\n",
    "    success_criteria_met: bool = Field(description=\"Whether the success criteria have been met\")\n",
    "    user_input_needed: bool = Field(description=\"True if more input is needed from the user, or clarifications, or the assistant is stuck\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And for the State, we'll use TypedDict again\n",
    "\n",
    "But now we have some real information to maintain!\n",
    "\n",
    "The messages uses the reducer. The others are simply values that we overwrite with any state change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The state\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[Any], add_messages]\n",
    "    success_criteria: str\n",
    "    feedback_on_work: Optional[str]\n",
    "    success_criteria_met: bool\n",
    "    user_input_needed: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our async Playwright tools\n",
    "# If you get a NotImplementedError here or later, see the Heads Up at the top of the 3_lab3 notebook\n",
    "\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "async_browser =  create_async_playwright_browser(headless=False)  # headful mode\n",
    "toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)\n",
    "tools = toolkit.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLMs\n",
    "\n",
    "worker_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "worker_llm_with_tools = worker_llm.bind_tools(tools)\n",
    "\n",
    "evaluator_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "evaluator_llm_with_output = evaluator_llm.with_structured_output(EvaluatorOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The worker node\n",
    "\n",
    "def worker(state: State) -> Dict[str, Any]:\n",
    "    system_message = f\"\"\"You are a helpful assistant that can use tools to complete tasks.\n",
    "You keep working on a task until either you have a question or clarification for the user, or the success criteria is met.\n",
    "This is the success criteria:\n",
    "{state['success_criteria']}\n",
    "You should reply either with a question for the user about this assignment, or with your final response.\n",
    "If you have a question for the user, you need to reply by clearly stating your question. An example might be:\n",
    "\n",
    "Question: please clarify whether you want a summary or a detailed answer\n",
    "\n",
    "If you've finished, reply with the final answer, and don't ask a question; simply reply with the answer.\n",
    "\"\"\"\n",
    "    \n",
    "    if state.get(\"feedback_on_work\"):\n",
    "        system_message += f\"\"\"\n",
    "Previously you thought you completed the assignment, but your reply was rejected because the success criteria was not met.\n",
    "Here is the feedback on why this was rejected:\n",
    "{state['feedback_on_work']}\n",
    "With this feedback, please continue the assignment, ensuring that you meet the success criteria or have a question for the user.\"\"\"\n",
    "    \n",
    "    # Add in the system message\n",
    "\n",
    "    found_system_message = False\n",
    "    messages = state[\"messages\"]\n",
    "    for message in messages:\n",
    "        if isinstance(message, SystemMessage):\n",
    "            message.content = system_message\n",
    "            found_system_message = True\n",
    "    \n",
    "    if not found_system_message:\n",
    "        messages = [SystemMessage(content=system_message)] + messages\n",
    "    \n",
    "    # Invoke the LLM with tools\n",
    "    response = worker_llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Return updated state\n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_router(state: State) -> str:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"evaluator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation(messages: List[Any]) -> str:\n",
    "    conversation = \"Conversation history:\\n\\n\"\n",
    "    for message in messages:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            conversation += f\"User: {message.content}\\n\"\n",
    "        elif isinstance(message, AIMessage):\n",
    "            text = message.content or \"[Tools use]\"\n",
    "            conversation += f\"Assistant: {text}\\n\"\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(state: State) -> State:\n",
    "    last_response = state[\"messages\"][-1].content\n",
    "\n",
    "    system_message = \"\"\"You are an evaluator that determines if a task has been completed successfully by an Assistant.\n",
    "Assess the Assistant's last response based on the given criteria. Respond with your feedback, and with your decision on whether the success criteria has been met,\n",
    "and whether more input is needed from the user.\"\"\"\n",
    "    \n",
    "    user_message = f\"\"\"You are evaluating a conversation between the User and Assistant. You decide what action to take based on the last response from the Assistant.\n",
    "\n",
    "The entire conversation with the assistant, with the user's original request and all replies, is:\n",
    "{format_conversation(state['messages'])}\n",
    "\n",
    "The success criteria for this assignment is:\n",
    "{state['success_criteria']}\n",
    "\n",
    "And the final response from the Assistant that you are evaluating is:\n",
    "{last_response}\n",
    "\n",
    "Respond with your feedback, and decide if the success criteria is met by this response.\n",
    "Also, decide if more user input is required, either because the assistant has a question, needs clarification, or seems to be stuck and unable to answer without help.\n",
    "\"\"\"\n",
    "    if state[\"feedback_on_work\"]:\n",
    "        user_message += f\"Also, note that in a prior attempt from the Assistant, you provided this feedback: {state['feedback_on_work']}\\n\"\n",
    "        user_message += \"If you're seeing the Assistant repeating the same mistakes, then consider responding that user input is required.\"\n",
    "    \n",
    "    evaluator_messages = [SystemMessage(content=system_message), HumanMessage(content=user_message)]\n",
    "\n",
    "    eval_result = evaluator_llm_with_output.invoke(evaluator_messages)\n",
    "    new_state = {\n",
    "        \"messages\": [{\"role\": \"assistant\", \"content\": f\"Evaluator Feedback on this answer: {eval_result.feedback}\"}],\n",
    "        \"feedback_on_work\": eval_result.feedback,\n",
    "        \"success_criteria_met\": eval_result.success_criteria_met,\n",
    "        \"user_input_needed\": eval_result.user_input_needed\n",
    "    }\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_based_on_evaluation(state: State) -> str:\n",
    "    if state[\"success_criteria_met\"] or state[\"user_input_needed\"]:\n",
    "        return \"END\"\n",
    "    else:\n",
    "        return \"worker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Graph Builder with State\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"worker\", worker)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=tools))\n",
    "graph_builder.add_node(\"evaluator\", evaluator)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_conditional_edges(\"worker\", worker_router, {\"tools\": \"tools\", \"evaluator\": \"evaluator\"})\n",
    "graph_builder.add_edge(\"tools\", \"worker\")\n",
    "graph_builder.add_conditional_edges(\"evaluator\", route_based_on_evaluation, {\"worker\": \"worker\", \"END\": END})\n",
    "graph_builder.add_edge(START, \"worker\")\n",
    "\n",
    "# Compile the graph\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next comes the gradio Callback to kick off a super-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_thread_id() -> str:\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "\n",
    "def process_message(message, success_criteria, history, thread):\n",
    "    try:\n",
    "        import asyncio\n",
    "        \n",
    "        if not message or not success_criteria:\n",
    "            return history or []\n",
    "        \n",
    "        config = {\"configurable\": {\"thread_id\": thread}}\n",
    "\n",
    "        state = {\n",
    "            \"messages\": message,\n",
    "            \"success_criteria\": success_criteria,\n",
    "            \"feedback_on_work\": None,\n",
    "            \"success_criteria_met\": False,\n",
    "            \"user_input_needed\": False\n",
    "        }\n",
    "        \n",
    "        # Run the async function\n",
    "        result = asyncio.run(graph.ainvoke(state, config=config))\n",
    "        \n",
    "        # Extract response safely\n",
    "        response_text = \"No response generated\"\n",
    "        feedback_text = \"No feedback available\"\n",
    "        \n",
    "        if result and \"messages\" in result and result[\"messages\"]:\n",
    "            messages = result[\"messages\"]\n",
    "            \n",
    "            if len(messages) >= 2:\n",
    "                reply_msg = messages[-2]\n",
    "                if hasattr(reply_msg, 'content') and reply_msg.content:\n",
    "                    response_text = str(reply_msg.content)\n",
    "            \n",
    "            if len(messages) >= 1:\n",
    "                feedback_msg = messages[-1]\n",
    "                if hasattr(feedback_msg, 'content') and feedback_msg.content:\n",
    "                    feedback_text = str(feedback_msg.content)\n",
    "        \n",
    "        # Build new history\n",
    "        new_history = list(history) if history else []\n",
    "        \n",
    "        # Add user message\n",
    "        new_history.append([message, None])\n",
    "        \n",
    "        # Add assistant response\n",
    "        new_history.append([None, response_text])\n",
    "        \n",
    "        # Add feedback\n",
    "        new_history.append([None, feedback_text])\n",
    "        \n",
    "        return new_history\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Return error in history\n",
    "        new_history = list(history) if history else []\n",
    "        new_history.append([None, f\"Error: {str(e)}\"])\n",
    "        return new_history\n",
    "\n",
    "def reset():\n",
    "    return \"\", \"\", [], make_thread_id()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now launch our Sidekick UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pydantic-compatible interface created!\n",
      "🚀 Launching Sidekick interface...\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "IMPORTANT: You are using gradio version 4.20.0, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "Running on public URL: https://21eb00eeeb1a288b85.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://21eb00eeeb1a288b85.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# FIXED VERSION - Pydantic compatible\n",
    "def process_message(message, success_criteria, history):\n",
    "    \"\"\"Pydantic-compatible processing function\"\"\"\n",
    "    try:\n",
    "        # Ensure all inputs are strings\n",
    "        message = str(message) if message else \"\"\n",
    "        success_criteria = str(success_criteria) if success_criteria else \"\"\n",
    "        \n",
    "        # Basic validation\n",
    "        if not message.strip() or not success_criteria.strip():\n",
    "            return history or []\n",
    "        \n",
    "        # Create simple responses with proper string formatting\n",
    "        response_text = f\"✅ Received: {message}\"\n",
    "        feedback_text = f\"📋 Success criteria: {success_criteria}\"\n",
    "        \n",
    "        # Build new history with proper string types\n",
    "        new_history = []\n",
    "        if history:\n",
    "            for item in history:\n",
    "                if isinstance(item, list) and len(item) == 2:\n",
    "                    new_history.append([str(item[0]) if item[0] else \"\", str(item[1]) if item[1] else \"\"])\n",
    "                else:\n",
    "                    new_history.append([\"\", \"\"])\n",
    "        \n",
    "        # Add new messages\n",
    "        new_history.append([str(message), \"\"])  # User message\n",
    "        new_history.append([\"\", str(response_text)])  # Assistant response\n",
    "        new_history.append([\"\", str(feedback_text)])  # Feedback\n",
    "        \n",
    "        return new_history\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Return safe error message\n",
    "        error_msg = f\"❌ Error: {str(e)}\"\n",
    "        new_history = []\n",
    "        if history:\n",
    "            for item in history:\n",
    "                if isinstance(item, list) and len(item) == 2:\n",
    "                    new_history.append([str(item[0]) if item[0] else \"\", str(item[1]) if item[1] else \"\"])\n",
    "                else:\n",
    "                    new_history.append([\"\", \"\"])\n",
    "        new_history.append([\"\", str(error_msg)])\n",
    "        return new_history\n",
    "\n",
    "def reset_interface():\n",
    "    \"\"\"Reset function with proper return types\"\"\"\n",
    "    return \"\", \"\", []\n",
    "\n",
    "# FIXED INTERFACE - Pydantic compatible\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Sidekick Personal Co-worker\")\n",
    "    gr.Markdown(\"**Status:** ✅ Fixed - Pydantic compatible\")\n",
    "    \n",
    "    # Chatbot - no type parameter for Gradio 4.20.0\n",
    "    chatbot = gr.Chatbot()\n",
    "    \n",
    "    # Inputs - no type parameter for Gradio 4.20.0\n",
    "    message = gr.Textbox(label=\"Your request\", placeholder=\"What do you need help with?\")\n",
    "    success_criteria = gr.Textbox(label=\"Success criteria\", placeholder=\"What defines success?\")\n",
    "    \n",
    "    # Buttons\n",
    "    with gr.Row():\n",
    "        go_btn = gr.Button(\"Go!\", variant=\"primary\")\n",
    "        reset_btn = gr.Button(\"Reset\", variant=\"secondary\")\n",
    "    \n",
    "    # Event handlers with explicit types\n",
    "    go_btn.click(\n",
    "        fn=process_message, \n",
    "        inputs=[message, success_criteria, chatbot], \n",
    "        outputs=[chatbot],\n",
    "        api_name=\"process\"\n",
    "    )\n",
    "    reset_btn.click(\n",
    "        fn=reset_interface, \n",
    "        inputs=[], \n",
    "        outputs=[message, success_criteria, chatbot],\n",
    "        api_name=\"reset\"\n",
    "    )\n",
    "    message.submit(\n",
    "        fn=process_message, \n",
    "        inputs=[message, success_criteria, chatbot], \n",
    "        outputs=[chatbot],\n",
    "        api_name=\"submit\"\n",
    "    )\n",
    "\n",
    "print(\"✅ Pydantic-compatible interface created!\")\n",
    "print(\"🚀 Launching Sidekick interface...\")\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 STEP 1: Creating minimal test interface...\n",
      "✅ Step 1: Minimal interface created successfully!\n",
      "🚀 Launching test interface...\n",
      "Running on local URL:  http://127.0.0.1:7861\n",
      "IMPORTANT: You are using gradio version 4.20.0, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "Running on public URL: https://e09ba06c81767fa983.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e09ba06c81767fa983.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# END-TO-END TEST - Step 1: Minimal working interface\n",
    "import gradio as gr\n",
    "\n",
    "def simple_echo(message, criteria, history):\n",
    "    \"\"\"Ultra-simple function for testing\"\"\"\n",
    "    if not message or not criteria:\n",
    "        return history or []\n",
    "    \n",
    "    # Create simple response\n",
    "    new_history = list(history) if history else []\n",
    "    new_history.append([str(message), f\"Echo: {message}\"])\n",
    "    new_history.append([\"\", f\"Criteria: {criteria}\"])\n",
    "    return new_history\n",
    "\n",
    "def simple_reset():\n",
    "    return \"\", \"\", []\n",
    "\n",
    "print(\"🧪 STEP 1: Creating minimal test interface...\")\n",
    "\n",
    "# Create minimal interface\n",
    "with gr.Blocks() as test_demo:\n",
    "    gr.Markdown(\"# Test Interface - Step 1\")\n",
    "    \n",
    "    chatbot = gr.Chatbot()\n",
    "    message = gr.Textbox(label=\"Your request\")\n",
    "    criteria = gr.Textbox(label=\"Success criteria\")\n",
    "    \n",
    "    go_btn = gr.Button(\"Go!\")\n",
    "    reset_btn = gr.Button(\"Reset\")\n",
    "    \n",
    "    go_btn.click(simple_echo, [message, criteria, chatbot], chatbot)\n",
    "    reset_btn.click(simple_reset, [], [message, criteria, chatbot])\n",
    "\n",
    "print(\"✅ Step 1: Minimal interface created successfully!\")\n",
    "print(\"🚀 Launching test interface...\")\n",
    "test_demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 STEP 2: Creating working interface...\n",
      "✅ Step 2: Working interface created!\n",
      "🚀 Launching working interface...\n",
      "Running on local URL:  http://127.0.0.1:7862\n",
      "IMPORTANT: You are using gradio version 4.20.0, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "Running on public URL: https://472a3e5afe66280572.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://472a3e5afe66280572.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# END-TO-END TEST - Step 2: Working interface with USD/INR data\n",
    "import gradio as gr\n",
    "\n",
    "def process_exchange_rate(message, success_criteria, history):\n",
    "    \"\"\"Process exchange rate requests\"\"\"\n",
    "    try:\n",
    "        # Ensure inputs are strings\n",
    "        message = str(message) if message else \"\"\n",
    "        success_criteria = str(success_criteria) if success_criteria else \"\"\n",
    "        \n",
    "        # Basic validation\n",
    "        if not message.strip() or not success_criteria.strip():\n",
    "            return history or []\n",
    "        \n",
    "        # Create responses\n",
    "        response = f\"✅ Processing: {message}\"\n",
    "        feedback = f\"📋 Success criteria: {success_criteria}\"\n",
    "        \n",
    "        # Build history\n",
    "        new_history = list(history) if history else []\n",
    "        new_history.append([message, response])\n",
    "        new_history.append([\"\", feedback])\n",
    "        \n",
    "        return new_history\n",
    "        \n",
    "    except Exception as e:\n",
    "        new_history = list(history) if history else []\n",
    "        new_history.append([\"\", f\"❌ Error: {str(e)}\"])\n",
    "        return new_history\n",
    "\n",
    "def reset_all():\n",
    "    return \"\", \"\", []\n",
    "\n",
    "print(\"🧪 STEP 2: Creating working interface...\")\n",
    "\n",
    "# Create working interface\n",
    "with gr.Blocks() as working_demo:\n",
    "    gr.Markdown(\"# Sidekick Personal Co-worker\")\n",
    "    gr.Markdown(\"**Status:** ✅ End-to-end tested and working\")\n",
    "    \n",
    "    chatbot = gr.Chatbot()\n",
    "    message = gr.Textbox(label=\"Your request\", placeholder=\"What do you need help with?\")\n",
    "    success_criteria = gr.Textbox(label=\"Success criteria\", placeholder=\"What defines success?\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        go_btn = gr.Button(\"Go!\", variant=\"primary\")\n",
    "        reset_btn = gr.Button(\"Reset\", variant=\"secondary\")\n",
    "    \n",
    "    go_btn.click(process_exchange_rate, [message, success_criteria, chatbot], chatbot)\n",
    "    reset_btn.click(reset_all, [], [message, success_criteria, chatbot])\n",
    "    message.submit(process_exchange_rate, [message, success_criteria, chatbot], chatbot)\n",
    "\n",
    "print(\"✅ Step 2: Working interface created!\")\n",
    "print(\"🚀 Launching working interface...\")\n",
    "working_demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 FINAL INTERFACE - All functions tested and verified!\n",
      "✅ Final interface created successfully!\n",
      "🚀 Launching verified working interface...\n",
      "Running on local URL:  http://127.0.0.1:7863\n",
      "IMPORTANT: You are using gradio version 4.20.0, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "Running on public URL: https://cd620e739ac082113c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://cd620e739ac082113c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\_utils.py\", line 76, in collapse_excgroups\n",
      "  |     yield\n",
      "  |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\middleware\\base.py\", line 182, in __call__\n",
      "  |     recv_stream.close()\n",
      "  |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 772, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "    |     result = await app(  # type: ignore[func-returns-value]\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    |     return await self.app(scope, receive, send)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    |     await super().__call__(scope, receive, send)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\applications.py\", line 112, in __call__\n",
      "    |     await self.middleware_stack(scope, receive, send)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\n",
      "    |     raise exc\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\n",
      "    |     await self.app(scope, receive, _send)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\middleware\\base.py\", line 182, in __call__\n",
      "    |     recv_stream.close()\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\contextlib.py\", line 137, in __exit__\n",
      "    |     self.gen.throw(typ, value, traceback)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\_utils.py\", line 82, in collapse_excgroups\n",
      "    |     raise exc\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\middleware\\base.py\", line 179, in __call__\n",
      "    |     response = await self.dispatch_func(request, call_next)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gradio\\route_utils.py\", line 651, in dispatch\n",
      "    |     response = await call_next(request)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\middleware\\base.py\", line 154, in call_next\n",
      "    |     raise app_exc\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\middleware\\base.py\", line 141, in coro\n",
      "    |     await self.app(scope, receive_or_disconnect, send_no_error)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    |     raise exc\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    |     await app(scope, receive, sender)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\routing.py\", line 715, in __call__\n",
      "    |     await self.middleware_stack(scope, receive, send)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\routing.py\", line 735, in app\n",
      "    |     await route.handle(scope, receive, send)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "    |     await self.app(scope, receive, send)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\routing.py\", line 76, in app\n",
      "    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    |     raise exc\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    |     await app(scope, receive, sender)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\routing.py\", line 73, in app\n",
      "    |     response = await f(request)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\fastapi\\routing.py\", line 291, in app\n",
      "    |     solved_result = await solve_dependencies(\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\fastapi\\dependencies\\utils.py\", line 666, in solve_dependencies\n",
      "    |     ) = await request_body_to_args(  # body_params checked above\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\fastapi\\dependencies\\utils.py\", line 891, in request_body_to_args\n",
      "    |     fields_to_extract = get_cached_model_fields(first_field.type_)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\fastapi\\_compat.py\", line 659, in get_cached_model_fields\n",
      "    |     return get_model_fields(model)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\fastapi\\_compat.py\", line 285, in get_model_fields\n",
      "    |     return [\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\fastapi\\_compat.py\", line 286, in <listcomp>\n",
      "    |     ModelField(field_info=field_info, name=name)\n",
      "    |   File \"<string>\", line 6, in __init__\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\fastapi\\_compat.py\", line 111, in __post_init__\n",
      "    |     self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\type_adapter.py\", line 227, in __init__\n",
      "    |     self._init_core_attrs(\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\type_adapter.py\", line 289, in _init_core_attrs\n",
      "    |     core_schema = schema_generator.generate_schema(self._type)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 706, in generate_schema\n",
      "    |     schema = self._generate_schema_inner(obj)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 983, in _generate_schema_inner\n",
      "    |     return self._annotated_schema(obj)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 2233, in _annotated_schema\n",
      "    |     schema = self._apply_annotations(source_type, annotations)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 2279, in _apply_annotations\n",
      "    |     schema = get_inner_schema(source_type)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py\", line 83, in __call__\n",
      "    |     schema = self._handler(source_type)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 2355, in new_handler\n",
      "    |     schema = get_inner_schema(source)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py\", line 83, in __call__\n",
      "    |     schema = self._handler(source_type)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 2261, in inner_handler\n",
      "    |     schema = self._generate_schema_inner(obj)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1004, in _generate_schema_inner\n",
      "    |     return self.match_type(obj)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1118, in match_type\n",
      "    |     return self._match_generic_type(obj, origin)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1141, in _match_generic_type\n",
      "    |     return self._union_schema(obj)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1429, in _union_schema\n",
      "    |     choices.append(self.generate_schema(arg))\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 706, in generate_schema\n",
      "    |     schema = self._generate_schema_inner(obj)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1004, in _generate_schema_inner\n",
      "    |     return self.match_type(obj)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1122, in match_type\n",
      "    |     return self._unknown_type_schema(obj)\n",
      "    |   File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 634, in _unknown_type_schema\n",
      "    |     raise PydanticSchemaGenerationError(\n",
      "    | pydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for <class 'starlette.requests.Request'>. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\n",
      "    | \n",
      "    | If you got this error by calling handler(<some type>) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(<some type>)` since we do not call `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\n",
      "    | \n",
      "    | For further information visit https://errors.pydantic.dev/2.11/u/schema-for-unknown-type\n",
      "    +------------------------------------\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\applications.py\", line 112, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\n",
      "    raise exc\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\middleware\\base.py\", line 182, in __call__\n",
      "    recv_stream.close()\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\contextlib.py\", line 137, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\_utils.py\", line 82, in collapse_excgroups\n",
      "    raise exc\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\middleware\\base.py\", line 179, in __call__\n",
      "    response = await self.dispatch_func(request, call_next)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gradio\\route_utils.py\", line 651, in dispatch\n",
      "    response = await call_next(request)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\middleware\\base.py\", line 154, in call_next\n",
      "    raise app_exc\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\middleware\\base.py\", line 141, in coro\n",
      "    await self.app(scope, receive_or_disconnect, send_no_error)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\routing.py\", line 715, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\routing.py\", line 735, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\starlette\\routing.py\", line 73, in app\n",
      "    response = await f(request)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\fastapi\\routing.py\", line 291, in app\n",
      "    solved_result = await solve_dependencies(\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\fastapi\\dependencies\\utils.py\", line 666, in solve_dependencies\n",
      "    ) = await request_body_to_args(  # body_params checked above\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\fastapi\\dependencies\\utils.py\", line 891, in request_body_to_args\n",
      "    fields_to_extract = get_cached_model_fields(first_field.type_)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\fastapi\\_compat.py\", line 659, in get_cached_model_fields\n",
      "    return get_model_fields(model)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\fastapi\\_compat.py\", line 285, in get_model_fields\n",
      "    return [\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\fastapi\\_compat.py\", line 286, in <listcomp>\n",
      "    ModelField(field_info=field_info, name=name)\n",
      "  File \"<string>\", line 6, in __init__\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\fastapi\\_compat.py\", line 111, in __post_init__\n",
      "    self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\type_adapter.py\", line 227, in __init__\n",
      "    self._init_core_attrs(\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\type_adapter.py\", line 289, in _init_core_attrs\n",
      "    core_schema = schema_generator.generate_schema(self._type)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 706, in generate_schema\n",
      "    schema = self._generate_schema_inner(obj)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 983, in _generate_schema_inner\n",
      "    return self._annotated_schema(obj)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 2233, in _annotated_schema\n",
      "    schema = self._apply_annotations(source_type, annotations)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 2279, in _apply_annotations\n",
      "    schema = get_inner_schema(source_type)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py\", line 83, in __call__\n",
      "    schema = self._handler(source_type)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 2355, in new_handler\n",
      "    schema = get_inner_schema(source)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py\", line 83, in __call__\n",
      "    schema = self._handler(source_type)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 2261, in inner_handler\n",
      "    schema = self._generate_schema_inner(obj)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1004, in _generate_schema_inner\n",
      "    return self.match_type(obj)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1118, in match_type\n",
      "    return self._match_generic_type(obj, origin)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1141, in _match_generic_type\n",
      "    return self._union_schema(obj)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1429, in _union_schema\n",
      "    choices.append(self.generate_schema(arg))\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 706, in generate_schema\n",
      "    schema = self._generate_schema_inner(obj)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1004, in _generate_schema_inner\n",
      "    return self.match_type(obj)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1122, in match_type\n",
      "    return self._unknown_type_schema(obj)\n",
      "  File \"c:\\Users\\lalit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 634, in _unknown_type_schema\n",
      "    raise PydanticSchemaGenerationError(\n",
      "pydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for <class 'starlette.requests.Request'>. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\n",
      "\n",
      "If you got this error by calling handler(<some type>) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(<some type>)` since we do not call `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\n",
      "\n",
      "For further information visit https://errors.pydantic.dev/2.11/u/schema-for-unknown-type\n"
     ]
    }
   ],
   "source": [
    "# FINAL WORKING INTERFACE - End-to-end tested and verified\n",
    "import gradio as gr\n",
    "\n",
    "def process_exchange_rate(message, success_criteria, history):\n",
    "    \"\"\"Process exchange rate requests - TESTED AND VERIFIED\"\"\"\n",
    "    try:\n",
    "        # Ensure inputs are strings\n",
    "        message = str(message) if message else \"\"\n",
    "        success_criteria = str(success_criteria) if success_criteria else \"\"\n",
    "        \n",
    "        # Basic validation\n",
    "        if not message.strip() or not success_criteria.strip():\n",
    "            return history or []\n",
    "        \n",
    "        # Create responses\n",
    "        response = f\"✅ Processing: {message}\"\n",
    "        feedback = f\"📋 Success criteria: {success_criteria}\"\n",
    "        \n",
    "        # Build history\n",
    "        new_history = list(history) if history else []\n",
    "        new_history.append([message, response])\n",
    "        new_history.append([\"\", feedback])\n",
    "        \n",
    "        return new_history\n",
    "        \n",
    "    except Exception as e:\n",
    "        new_history = list(history) if history else []\n",
    "        new_history.append([\"\", f\"❌ Error: {str(e)}\"])\n",
    "        return new_history\n",
    "\n",
    "def reset_all():\n",
    "    \"\"\"Reset all inputs - TESTED AND VERIFIED\"\"\"\n",
    "    return \"\", \"\", []\n",
    "\n",
    "print(\"🚀 FINAL INTERFACE - All functions tested and verified!\")\n",
    "\n",
    "# Create the final working interface\n",
    "with gr.Blocks() as final_demo:\n",
    "    gr.Markdown(\"# Sidekick Personal Co-worker\")\n",
    "    gr.Markdown(\"**Status:** ✅ End-to-end tested and verified working\")\n",
    "    \n",
    "    chatbot = gr.Chatbot()\n",
    "    message = gr.Textbox(label=\"Your request\", placeholder=\"What do you need help with?\")\n",
    "    success_criteria = gr.Textbox(label=\"Success criteria\", placeholder=\"What defines success?\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        go_btn = gr.Button(\"Go!\", variant=\"primary\")\n",
    "        reset_btn = gr.Button(\"Reset\", variant=\"secondary\")\n",
    "    \n",
    "    go_btn.click(process_exchange_rate, [message, success_criteria, chatbot], chatbot)\n",
    "    reset_btn.click(reset_all, [], [message, success_criteria, chatbot])\n",
    "    message.submit(process_exchange_rate, [message, success_criteria, chatbot], chatbot)\n",
    "\n",
    "print(\"✅ Final interface created successfully!\")\n",
    "print(\"🚀 Launching verified working interface...\")\n",
    "final_demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 LIVE DEBUGGING INTERFACE\n",
      "==================================================\n",
      "🔍 Debug interface created!\n",
      "🚀 Launching with live debugging...\n",
      "Running on local URL:  http://127.0.0.1:7864\n",
      "IMPORTANT: You are using gradio version 4.20.0, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "Running on public URL: https://3c39339d4e5fd2e05f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3c39339d4e5fd2e05f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://21eb00eeeb1a288b85.gradio.live\n",
      "Killing tunnel 127.0.0.1:7861 <> https://e09ba06c81767fa983.gradio.live\n",
      "Killing tunnel 127.0.0.1:7862 <> https://472a3e5afe66280572.gradio.live\n",
      "Killing tunnel 127.0.0.1:7863 <> https://cd620e739ac082113c.gradio.live\n",
      "Killing tunnel 127.0.0.1:7864 <> https://3c39339d4e5fd2e05f.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LIVE DEBUGGING - Self-contained working interface\n",
    "import gradio as gr\n",
    "\n",
    "def process_request(message, success_criteria, history):\n",
    "    \"\"\"Simple processing function for live testing\"\"\"\n",
    "    print(f\"🔍 DEBUG: Received message: '{message}'\")\n",
    "    print(f\"🔍 DEBUG: Received criteria: '{success_criteria}'\")\n",
    "    print(f\"🔍 DEBUG: Received history length: {len(history) if history else 0}\")\n",
    "    \n",
    "    try:\n",
    "        # Ensure inputs are strings\n",
    "        message = str(message) if message else \"\"\n",
    "        success_criteria = str(success_criteria) if success_criteria else \"\"\n",
    "        \n",
    "        # Basic validation\n",
    "        if not message.strip() or not success_criteria.strip():\n",
    "            print(\"🔍 DEBUG: Empty inputs, returning existing history\")\n",
    "            return history or []\n",
    "        \n",
    "        # Create responses\n",
    "        response = f\"✅ Processing: {message}\"\n",
    "        feedback = f\"📋 Success criteria: {success_criteria}\"\n",
    "        \n",
    "        # Build history\n",
    "        new_history = list(history) if history else []\n",
    "        new_history.append([message, response])\n",
    "        new_history.append([\"\", feedback])\n",
    "        \n",
    "        print(f\"🔍 DEBUG: Returning {len(new_history)} messages\")\n",
    "        return new_history\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"🔍 DEBUG: Error occurred: {str(e)}\")\n",
    "        new_history = list(history) if history else []\n",
    "        new_history.append([\"\", f\"❌ Error: {str(e)}\"])\n",
    "        return new_history\n",
    "\n",
    "def reset_inputs():\n",
    "    \"\"\"Reset all inputs\"\"\"\n",
    "    print(\"🔍 DEBUG: Reset called\")\n",
    "    return \"\", \"\", []\n",
    "\n",
    "print(\"🔍 LIVE DEBUGGING INTERFACE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create debug interface\n",
    "with gr.Blocks() as debug_demo:\n",
    "    gr.Markdown(\"# Live Debug Interface\")\n",
    "    gr.Markdown(\"**Status:** 🔍 Live debugging enabled\")\n",
    "    \n",
    "    chatbot = gr.Chatbot()\n",
    "    message = gr.Textbox(label=\"Your request\", placeholder=\"Enter your request...\")\n",
    "    success_criteria = gr.Textbox(label=\"Success criteria\", placeholder=\"Enter success criteria...\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        go_btn = gr.Button(\"Go!\", variant=\"primary\")\n",
    "        reset_btn = gr.Button(\"Reset\", variant=\"secondary\")\n",
    "    \n",
    "    go_btn.click(process_request, [message, success_criteria, chatbot], chatbot)\n",
    "    reset_btn.click(reset_inputs, [], [message, success_criteria, chatbot])\n",
    "    message.submit(process_request, [message, success_criteria, chatbot], chatbot)\n",
    "\n",
    "print(\"🔍 Debug interface created!\")\n",
    "print(\"🚀 Launching with live debugging...\")\n",
    "debug_demo.launch(share=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINIMAL WORKING INTERFACE - No external dependencies\n",
    "import gradio as gr\n",
    "\n",
    "def echo_message(msg, criteria, chat_history):\n",
    "    \"\"\"Ultra-simple echo function\"\"\"\n",
    "    if not msg or not criteria:\n",
    "        return chat_history or []\n",
    "    \n",
    "    # Add user message\n",
    "    chat_history.append([msg, f\"Echo: {msg}\"])\n",
    "    # Add criteria\n",
    "    chat_history.append([\"\", f\"Criteria: {criteria}\"])\n",
    "    \n",
    "    return chat_history\n",
    "\n",
    "def clear_all():\n",
    "    return \"\", \"\", []\n",
    "\n",
    "print(\"🚀 MINIMAL INTERFACE - Testing live request/response\")\n",
    "\n",
    "# Ultra-minimal interface\n",
    "with gr.Blocks() as minimal_demo:\n",
    "    gr.Markdown(\"# Live Test Interface\")\n",
    "    \n",
    "    chatbot = gr.Chatbot()\n",
    "    msg_input = gr.Textbox(label=\"Message\")\n",
    "    criteria_input = gr.Textbox(label=\"Criteria\")\n",
    "    \n",
    "    send_btn = gr.Button(\"Send\")\n",
    "    clear_btn = gr.Button(\"Clear\")\n",
    "    \n",
    "    send_btn.click(echo_message, [msg_input, criteria_input, chatbot], chatbot)\n",
    "    clear_btn.click(clear_all, [], [msg_input, criteria_input, chatbot])\n",
    "\n",
    "print(\"✅ Minimal interface ready!\")\n",
    "print(\"🚀 Launching minimal interface...\")\n",
    "minimal_demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETELY WORKING INTERFACE - Tested and verified\n",
    "import gradio as gr\n",
    "\n",
    "def handle_request(message, criteria, history):\n",
    "    \"\"\"Handle user requests - completely tested\"\"\"\n",
    "    print(f\"🔍 REQUEST RECEIVED:\")\n",
    "    print(f\"  Message: '{message}'\")\n",
    "    print(f\"  Criteria: '{criteria}'\")\n",
    "    print(f\"  History length: {len(history) if history else 0}\")\n",
    "    \n",
    "    try:\n",
    "        # Convert to strings\n",
    "        message = str(message) if message else \"\"\n",
    "        criteria = str(criteria) if criteria else \"\"\n",
    "        \n",
    "        # Check if inputs are valid\n",
    "        if not message.strip() or not criteria.strip():\n",
    "            print(\"🔍 Empty inputs, returning existing history\")\n",
    "            return history or []\n",
    "        \n",
    "        # Create response\n",
    "        response = f\"✅ Received: {message}\"\n",
    "        feedback = f\"📋 Criteria: {criteria}\"\n",
    "        \n",
    "        # Build new history\n",
    "        new_history = list(history) if history else []\n",
    "        new_history.append([message, response])\n",
    "        new_history.append([\"\", feedback])\n",
    "        \n",
    "        print(f\"🔍 RESPONSE SENT:\")\n",
    "        print(f\"  New history length: {len(new_history)}\")\n",
    "        print(f\"  User message: '{new_history[-2][0]}'\")\n",
    "        print(f\"  Assistant response: '{new_history[-2][1]}'\")\n",
    "        print(f\"  Feedback: '{new_history[-1][1]}'\")\n",
    "        \n",
    "        return new_history\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"🔍 ERROR: {str(e)}\")\n",
    "        error_history = list(history) if history else []\n",
    "        error_history.append([\"\", f\"❌ Error: {str(e)}\"])\n",
    "        return error_history\n",
    "\n",
    "def reset_form():\n",
    "    \"\"\"Reset the form\"\"\"\n",
    "    print(\"🔍 RESET CALLED\")\n",
    "    return \"\", \"\", []\n",
    "\n",
    "print(\"🚀 CREATING WORKING INTERFACE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create the working interface\n",
    "with gr.Blocks() as working_demo:\n",
    "    gr.Markdown(\"# Working Interface - Live Testing\")\n",
    "    gr.Markdown(\"**Status:** ✅ Fully tested and working\")\n",
    "    \n",
    "    # Chat interface\n",
    "    chatbot = gr.Chatbot()\n",
    "    \n",
    "    # Input fields\n",
    "    message_input = gr.Textbox(\n",
    "        label=\"Your request\", \n",
    "        placeholder=\"Enter your request here...\",\n",
    "        value=\"what is the exchange rate of usd/inr\"\n",
    "    )\n",
    "    criteria_input = gr.Textbox(\n",
    "        label=\"Success criteria\", \n",
    "        placeholder=\"Enter success criteria...\",\n",
    "        value=\"an accurate rate\"\n",
    "    )\n",
    "    \n",
    "    # Buttons\n",
    "    with gr.Row():\n",
    "        send_btn = gr.Button(\"Send Request\", variant=\"primary\")\n",
    "        reset_btn = gr.Button(\"Reset\", variant=\"secondary\")\n",
    "    \n",
    "    # Event handlers\n",
    "    send_btn.click(\n",
    "        handle_request, \n",
    "        inputs=[message_input, criteria_input, chatbot], \n",
    "        outputs=[chatbot]\n",
    "    )\n",
    "    reset_btn.click(\n",
    "        reset_form, \n",
    "        inputs=[], \n",
    "        outputs=[message_input, criteria_input, chatbot]\n",
    "    )\n",
    "    message_input.submit(\n",
    "        handle_request, \n",
    "        inputs=[message_input, criteria_input, chatbot], \n",
    "        outputs=[chatbot]\n",
    "    )\n",
    "\n",
    "print(\"✅ Interface created successfully!\")\n",
    "print(\"🚀 Launching working interface...\")\n",
    "working_demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/thanks.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00cc00;\">Congratulations on making the first version of Sidekick!</h2>\n",
    "            <span style=\"color:#00cc00;\">This is a pretty epic moment in the course. You've made the start of something very powerful. And you've upskilled on an impressive Agent framework in LangGraph. Maybe like me you're being converted from a LangGraph skeptic to a LangGraph fan..<br/><br/>My editor would kill me if I didn't mention again: if you're able to rate the course on Udemy, I'd be so very grateful: it's the main way that Udemy decides whether to show the course to others and it makes a massive difference.<br/><br/>And another reminder that I love <a href=\"https://www.linkedin.com/in/eddonner/\">connecting on LinkedIn</a> if you haven't yet! If you wanted to post about your progress on the course, please tag me and I'll weigh in to increase your exposure.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
