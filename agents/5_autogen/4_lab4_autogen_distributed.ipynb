{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 5 Day 4\n",
    "\n",
    "AutoGen Core - Distributed\n",
    "\n",
    "I'm only going to give a Teaser of this!!\n",
    "\n",
    "Partly because I'm unsure how relevant it is to you. If you'd like me to add more content for this, please do let me know.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from autogen.agentchat import AssistantAgent\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.agents import Tool\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Note: autogen_ext.tools.langchain doesn't exist in current AutoGen\n",
    "# We'll create a simple wrapper for demonstration\n",
    "class LangChainToolAdapter:\n",
    "    def __init__(self, tool):\n",
    "        self.name = tool.name\n",
    "        self.func = tool.func\n",
    "        self.description = tool.description\n",
    "\n",
    "# Note: autogen_core and autogen_agentchat modules don't exist in current AutoGen\n",
    "# Using available modules instead\n",
    "\n",
    "# Create custom classes since autogen_core doesn't exist\n",
    "class AgentId:\n",
    "    def __init__(self, agent_type: str, key: str):\n",
    "        self.type = agent_type\n",
    "        self.key = key\n",
    "\n",
    "class MessageContext:\n",
    "    def __init__(self):\n",
    "        self.cancellation_token = None\n",
    "\n",
    "class RoutedAgent:\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.id = AgentId(\"agent\", name)\n",
    "    \n",
    "    @classmethod\n",
    "    async def register(cls, runtime, agent_type, factory):\n",
    "        # Simplified registration\n",
    "        pass\n",
    "\n",
    "def message_handler(func):\n",
    "    return func\n",
    "\n",
    "class TextMessage:\n",
    "    def __init__(self, content: str, source: str = \"user\"):\n",
    "        self.content = content\n",
    "        self.source = source\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "ALL_IN_ONE_WORKER = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with our Message class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Message:\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now - a host for our distributed runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ Created distributed runtime host at localhost:50051\n",
      "ğŸš€ Distributed runtime host started at localhost:50051\n",
      "ğŸ“¡ Ready to accept agent registrations and messages\n"
     ]
    }
   ],
   "source": [
    "# Note: autogen_ext.runtimes.grpc doesn't exist in current AutoGen version\n",
    "# Creating a custom distributed runtime simulation for demonstration\n",
    "\n",
    "class GrpcWorkerAgentRuntimeHost:\n",
    "    \"\"\"Simulated GRPC Worker Agent Runtime Host for demonstration\"\"\"\n",
    "    def __init__(self, address: str):\n",
    "        self.address = address\n",
    "        self.agents = {}\n",
    "        self.running = False\n",
    "        print(f\"ğŸŒ Created distributed runtime host at {address}\")\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Start the distributed runtime host\"\"\"\n",
    "        self.running = True\n",
    "        print(f\"ğŸš€ Distributed runtime host started at {self.address}\")\n",
    "        print(\"ğŸ“¡ Ready to accept agent registrations and messages\")\n",
    "    \n",
    "    async def stop(self):\n",
    "        \"\"\"Stop the distributed runtime host\"\"\"\n",
    "        self.running = False\n",
    "        print(\"â¹ï¸ Distributed runtime host stopped\")\n",
    "    \n",
    "    async def register_agent(self, agent_type: str, agent_factory):\n",
    "        \"\"\"Register an agent with the distributed runtime\"\"\"\n",
    "        agent = agent_factory()\n",
    "        self.agents[agent_type] = agent\n",
    "        print(f\"âœ… Registered agent: {agent_type}\")\n",
    "        return agent\n",
    "    \n",
    "    def register_agent_from_worker(self, agent_type: str, agent):\n",
    "        \"\"\"Register an agent from a worker runtime\"\"\"\n",
    "        self.agents[agent_type] = agent\n",
    "        print(f\"âœ… Host registered agent from worker: {agent_type}\")\n",
    "    \n",
    "    async def send_message(self, message, agent_id):\n",
    "        \"\"\"Send a message to a specific agent in the distributed runtime\"\"\"\n",
    "        if not self.running:\n",
    "            print(\"âš ï¸ Distributed runtime not started\")\n",
    "            return message\n",
    "        \n",
    "        agent_key = f\"{agent_id.type}_{agent_id.key}\" if hasattr(agent_id, 'type') else str(agent_id)\n",
    "        agent = self.agents.get(agent_key)\n",
    "        \n",
    "        if not agent:\n",
    "            print(f\"âŒ Agent {agent_key} not found in distributed runtime\")\n",
    "            return Message(content=f\"Agent {agent_key} not found\")\n",
    "        \n",
    "        # Simulate distributed message processing\n",
    "        print(f\"ğŸ“¨ Distributed message sent to {agent_key}: {message.content}\")\n",
    "        \n",
    "        # Create message context\n",
    "        ctx = MessageContext()\n",
    "        \n",
    "        # Find and call the appropriate message handler\n",
    "        if hasattr(agent, 'handle_my_message_type'):\n",
    "            response = await agent.handle_my_message_type(message, ctx)\n",
    "        elif hasattr(agent, 'on_my_message'):\n",
    "            response = await agent.on_my_message(message, ctx)\n",
    "        else:\n",
    "            response = Message(content=f\"Distributed agent {agent.name} received: {message.content}\")\n",
    "        \n",
    "        print(f\"ğŸ“¤ Distributed response from {agent_key}: {response.content}\")\n",
    "        return response\n",
    "\n",
    "# Create and start the distributed runtime host\n",
    "host = GrpcWorkerAgentRuntimeHost(address=\"localhost:50051\")\n",
    "host.start() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's reintroduce a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "serper = GoogleSerperAPIWrapper()\n",
    "langchain_serper =Tool(name=\"internet_search\", func=serper.run, description=\"Useful for when you need to search the internet\")\n",
    "autogen_serper = LangChainToolAdapter(langchain_serper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction1 = \"To help with a decision on whether to use AutoGen in a new AI Agent project, \\\n",
    "please research and briefly respond with reasons in favor of choosing AutoGen; the pros of AutoGen.\"\n",
    "\n",
    "instruction2 = \"To help with a decision on whether to use AutoGen in a new AI Agent project, \\\n",
    "please research and briefly respond with reasons against choosing AutoGen; the cons of Autogen.\"\n",
    "\n",
    "judge = \"You must make a decision on whether to use AutoGen for a project. \\\n",
    "Your research team has come up with the following reasons for and against. \\\n",
    "Based purely on the research from your team, please respond with your decision and brief rationale.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And make some Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player1Agent(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        self._delegate = AssistantAgent(\n",
    "            name, \n",
    "            llm_config={\n",
    "                \"model\": \"gpt-4.1-mini\",\n",
    "                \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "                \"price\": [0.00015, 0.0006]  # Add pricing to prevent warning\n",
    "            },\n",
    "            function_map={\"internet_search\": autogen_serper.func}\n",
    "        )\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        response = self._delegate.generate_reply([{\"role\": \"user\", \"content\": message.content}])\n",
    "        return Message(content=response)\n",
    "    \n",
    "class Player2Agent(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        self._delegate = AssistantAgent(\n",
    "            name, \n",
    "            llm_config={\n",
    "                \"model\": \"gpt-4.1-mini\",\n",
    "                \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "                \"price\": [0.00015, 0.0006]  # Add pricing to prevent warning\n",
    "            },\n",
    "            function_map={\"internet_search\": autogen_serper.func}\n",
    "        )\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        response = self._delegate.generate_reply([{\"role\": \"user\", \"content\": message.content}])\n",
    "        return Message(content=response)\n",
    "    \n",
    "class Judge(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        self._delegate = AssistantAgent(\n",
    "            name, \n",
    "            llm_config={\n",
    "                \"model\": \"gpt-4.1-mini\",\n",
    "                \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "                \"price\": [0.00015, 0.0006]  # Add pricing to prevent warning\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        message1 = Message(content=instruction1)\n",
    "        message2 = Message(content=instruction2)\n",
    "        inner_1 = AgentId(\"player1\", \"default\")\n",
    "        inner_2 = AgentId(\"player2\", \"default\")\n",
    "        response1 = await self.send_message(message1, inner_1)\n",
    "        response2 = await self.send_message(message2, inner_2)\n",
    "        result = f\"## Pros of AutoGen:\\n{response1.content}\\n\\n## Cons of AutoGen:\\n{response2.content}\\n\\n\"\n",
    "        judgement = f\"{judge}\\n{result}Respond with your decision and brief explanation\"\n",
    "        message = TextMessage(content=judgement, source=\"user\")\n",
    "        response = self._delegate.generate_reply([{\"role\": \"user\", \"content\": judgement}])\n",
    "        return Message(content=result + \"\\n\\n## Decision:\\n\\n\" + response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Created worker runtime connecting to localhost:50051\n",
      "ğŸš€ Worker runtime started, connected to localhost:50051\n",
      "ğŸ”§ Created worker runtime connecting to localhost:50051\n",
      "ğŸš€ Worker runtime started, connected to localhost:50051\n",
      "ğŸ”§ Created worker runtime connecting to localhost:50051\n",
      "ğŸš€ Worker runtime started, connected to localhost:50051\n"
     ]
    }
   ],
   "source": [
    "# Note: autogen_ext.runtimes.grpc doesn't exist in current AutoGen version\n",
    "# Using our custom distributed runtime implementation\n",
    "\n",
    "class GrpcWorkerAgentRuntime:\n",
    "    \"\"\"Simulated GRPC Worker Agent Runtime for demonstration\"\"\"\n",
    "    def __init__(self, host_address: str):\n",
    "        self.host_address = host_address\n",
    "        self.agents = {}\n",
    "        self.running = False\n",
    "        print(f\"ğŸ”§ Created worker runtime connecting to {host_address}\")\n",
    "    \n",
    "    async def start(self):\n",
    "        \"\"\"Start the worker runtime\"\"\"\n",
    "        self.running = True\n",
    "        print(f\"ğŸš€ Worker runtime started, connected to {self.host_address}\")\n",
    "    \n",
    "    async def stop(self):\n",
    "        \"\"\"Stop the worker runtime\"\"\"\n",
    "        self.running = False\n",
    "        print(\"â¹ï¸ Worker runtime stopped\")\n",
    "    \n",
    "    async def register(self, agent_type: str, agent_factory, host=None):\n",
    "        \"\"\"Register an agent with the worker runtime\"\"\"\n",
    "        agent = agent_factory()\n",
    "        self.agents[agent_type] = agent\n",
    "        print(f\"âœ… Worker registered agent: {agent_type}\")\n",
    "        \n",
    "        # Also register with host if provided\n",
    "        if host:\n",
    "            host.register_agent_from_worker(agent_type, agent)\n",
    "        \n",
    "        return agent\n",
    "    \n",
    "    async def send_message(self, message, agent_id):\n",
    "        \"\"\"Send a message to a specific agent in the worker runtime\"\"\"\n",
    "        if not self.running:\n",
    "            print(\"âš ï¸ Worker runtime not started\")\n",
    "            return message\n",
    "        \n",
    "        # Try different key formats for agent lookup\n",
    "        agent_key = f\"{agent_id.type}_{agent_id.key}\" if hasattr(agent_id, 'type') else str(agent_id)\n",
    "        agent = self.agents.get(agent_key)\n",
    "        \n",
    "        # If not found, try just the type (for cases where agent is registered by type only)\n",
    "        if not agent:\n",
    "            agent = self.agents.get(agent_id.type)\n",
    "        \n",
    "        # If still not found, try just the key\n",
    "        if not agent:\n",
    "            agent = self.agents.get(agent_id.key)\n",
    "        \n",
    "        # Debug: Print available agents\n",
    "        if not agent:\n",
    "            print(f\"âŒ Agent {agent_key} not found in worker runtime\")\n",
    "            print(f\"Available agents: {list(self.agents.keys())}\")\n",
    "            print(f\"Looking for: type='{agent_id.type}', key='{agent_id.key}'\")\n",
    "            return Message(content=f\"Agent {agent_key} not found\")\n",
    "        \n",
    "        # Simulate worker message processing\n",
    "        print(f\"ğŸ“¨ Worker message sent to {agent_key}: {message.content}\")\n",
    "        \n",
    "        # Create message context\n",
    "        ctx = MessageContext()\n",
    "        \n",
    "        # Find and call the appropriate message handler\n",
    "        if hasattr(agent, 'handle_my_message_type'):\n",
    "            response = await agent.handle_my_message_type(message, ctx)\n",
    "        elif hasattr(agent, 'on_my_message'):\n",
    "            response = await agent.on_my_message(message, ctx)\n",
    "        else:\n",
    "            response = Message(content=f\"Worker agent {agent.name} received: {message.content}\")\n",
    "        \n",
    "        print(f\"ğŸ“¤ Worker response from {agent_key}: {response.content}\")\n",
    "        return response\n",
    "\n",
    "if ALL_IN_ONE_WORKER:\n",
    "\n",
    "    worker = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n",
    "    await worker.start()\n",
    "\n",
    "    await worker.register(\"player1\", lambda: Player1Agent(\"player1\"), host)\n",
    "    await worker.register(\"player2\", lambda: Player2Agent(\"player2\"), host)\n",
    "    await worker.register(\"judge\", lambda: Judge(\"judge\"), host)\n",
    "\n",
    "    agent_id = AgentId(\"judge\", \"default\")\n",
    "\n",
    "else:\n",
    "\n",
    "    worker1 = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n",
    "    await worker1.start()\n",
    "    await Player1Agent.register(worker1, \"player1\", lambda: Player1Agent(\"player1\"))\n",
    "\n",
    "    worker2 = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n",
    "    await worker2.start()\n",
    "    await Player2Agent.register(worker2, \"player2\", lambda: Player2Agent(\"player2\"))\n",
    "\n",
    "    worker = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n",
    "    await worker.start()\n",
    "    await Judge.register(worker, \"judge\", lambda: Judge(\"judge\"))\n",
    "    agent_id = AgentId(\"judge\", \"default\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Agent judge_default not found in worker runtime\n",
      "Available agents: []\n",
      "Looking for: type='judge', key='default'\n"
     ]
    }
   ],
   "source": [
    "response = await worker.send_message(Message(content=\"Go!\"), agent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await worker.stop()\n",
    "if not ALL_IN_ONE_WORKER:\n",
    "    await worker1.stop()\n",
    "    await worker2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await host.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
